{"cells":[{"cell_type":"code","source":["%run ./setAccess"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d920b3c1-5b09-4635-a460-2564c494a100"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./cleaningBase"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6449dbc-05d1-442a-911b-d02300ea765b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./cleaningFunctions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b496756-76e7-4bf5-aa8e-a722eef8c18a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# passed by pipeline (user defined) parameters (set for specific database)\nrawFileNamePattern = dbutils.widgets.get(\"rawFileNamePattern\")\nconfigFileName = dbutils.widgets.get(\"configFileName\")\n\n# passed by pipeline (trigger) parameters\nfileName = dbutils.widgets.get(\"fileName\")\nfilePath = dbutils.widgets.get(\"filePath\")  \n\n# pre-validate the file\ncheckIfNotRejectedDir(filePath)\ncheckFileNamePattern(fileName,rawFileNamePattern)\ncheckHeader(fileName)\n\n# load config for database\nconfig = (spark.read\n          .format(\"json\")\n          .option(\"multiline\",\"true\")\n          .load(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+\n                dirs[\"config\"]+configFileName)\n          .collect()\n         )\n\n# load file for cleaning\ndata = (spark.read.format(\"csv\")\n        .option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"mode\",\"dropmalformed\")\n        .load(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+\n              dirs[\"lvl1\"]+fileName)\n       )\n\n# identify labels of columns in loaded data matching the config assumptions\ndata = getColumnsForCleaning(config,data)\n\n# impose a schema based on config assumptions\ndata = spark.createDataFrame(\n    data.select([conf.asDict()[\"targetName\"] for conf in config]).rdd,\n    schema = buildSchema(config)\n)\n\n# remove duplicates - based on \"primaryKeys\" identified in config\ndata = data.dropDuplicates([conf.asDict()[\"targetName\"] for conf in config if conf.asDict()[\"primaryKey\"]])\n\n# apply config-specified cleaning functions (see: Notebook: cleaningFunctions) for chosen columns\nfrom pyspark.sql.functions import trim,col\nfor conf in config:\n    colName = conf.asDict()[\"targetName\"]\n    if conf.asDict()[\"type\"]==\"String\":\n        data = (data.withColumn(colName, trim(col(colName)))\n               .fillna(\"NA\",subset=colName))\n    else:\n        data = data.fillna(0,subset=colName)\n    for function in conf.asDict()[\"applyFunctions\"]:\n        data = func[function[0]](\n            colName,\n            data,\n            eval(function[1])\n        )\n    \n(data.write\n    .format(\"parquet\")\n    .mode(\"overwrite\")\n    .option(\"header\", \"true\")\n    .save(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+\n          dirs[\"lvl2\"]+fileName.split(\".\")[0]+\".parquet\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43a1546b-eacb-4cdc-bb7f-6ceeee4d0604"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"transform","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1260026330619448}},"nbformat":4,"nbformat_minor":0}
