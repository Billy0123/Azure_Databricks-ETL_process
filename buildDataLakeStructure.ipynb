{"cells":[{"cell_type":"code","source":["%run ./setAccess"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3371044d-5787-4120-acb7-da34faf31602"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# create container with all ETL process levels and usages\nspark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\ndbutils.fs.ls(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\")\nspark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")\n\nfor dir in dirs.values():\n    dbutils.fs.mkdirs(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+dir)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3698f6a-7ca9-475c-94a5-90670ea85266"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# build a star-shaped database in \"curated\" Data Lake's layer using .json config file\nfrom pyspark.sql.types import *\n\nconfigFileName = \"airlines.json\"\nconfig = (spark.read\n          .format(\"json\")\n          .option(\"multiline\",\"true\")\n          .load(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+\n                dirs[\"config\"]+configFileName)\n          .collect()\n         )\n\ndatabases = list(set([conf.asDict()[\"targetBase\"] for conf in config]))\nmapTypes = {\n    \"Integer\": IntegerType(),\n    \"Long\": LongType(),\n    \"String\": StringType(),\n}\n# create empty dimension tables \nfor database in databases:\n    colNames = [conf.asDict()[\"targetName\"] for conf in config if conf.asDict()[\"targetBase\"]==database]\n    types = [conf.asDict()[\"type\"] for conf in config if conf.asDict()[\"targetBase\"]==database]\n    nullable = [conf.asDict()[\"nullable\"] for conf in config if conf.asDict()[\"targetBase\"]==database]\n    schema = [StructField(\"ID\",LongType(),True)]\n    for cN,t,n in list(zip(colNames,types,nullable)):\n        schema.append(StructField(cN,mapTypes[t],n))\n    schema = StructType(schema)\n    df = spark.createDataFrame(data=[],schema=schema)\n    (df.write\n        .format(\"parquet\")\n        .mode(\"overwrite\")\n        .option(\"header\", \"true\")\n        .save(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+\n              dirs[\"lvl3\"]+database+\".parquet\"))\n\n# create empty facts table\nschema = [StructField(\"ID\",LongType(),True)]\nfor database in databases:\n    schema.append(StructField(database+\"_ID\",LongType(),True))\nschema = StructType(schema)\ndf = spark.createDataFrame(data=[],schema=schema)\n(df.write\n     .format(\"parquet\")\n     .mode(\"overwrite\")\n     .option(\"header\", \"true\")\n     .save(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+\n           dirs[\"lvl3\"]+configFileName.split(\".\")[0]+\".parquet\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4dd16c3-d85e-460b-873d-b74414aa999f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# create some random test files, using first 10 files from '/databricks-datasets/airlines/'\nfrom pyspark.sql.types import StructType\nimport random\nimport string\nfrom itertools import chain\nfrom pyspark.sql.functions import create_map,lit,col\n\nheader = (spark.read\n          .format(\"csv\")\n          .option(\"header\", \"true\")\n          .option(\"inferSchema\", \"true\")\n          .load(\"/databricks-datasets/airlines/part-00000\")\n)\nschema = header.schema  # header found only in (...)part-00000\ndata = (spark.read\n        .format(\"csv\")\n        .option(\"header\", \"true\")\n        .schema(schema)\n        .load(\"/databricks-datasets/airlines/part-0000*\")\n       )\n\nfiles = data.randomSplit([1.0 for i in range(10)], 1)\ndef createFiles(files,range_,fileNamePrefix,header):\n    for i in range_:\n        # create single-csv-files, not folders with 'parts'\n        (files[i].coalesce(1).write \n            .format(\"csv\") \n            .option(\"header\", header) \n            .mode(\"overwrite\") \n            .save(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+\n                  dirs[\"test\"]+fileNamePrefix+str(i)+\".csv\") \n        )\n        dbutils.fs.mv(\n            \"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+dirs[\"test\"]+fileNamePrefix+str(i)+\".csv\",\n            \"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+dirs[\"test\"]+fileNamePrefix+str(i)+\".csv_buff\",\n            recurse=True\n        )\n        fileList = dbutils.fs.ls(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+dirs[\"test\"]+fileNamePrefix+str(i)+\".csv_buff\")\n        fileList = [fileList[i][0] for i in range(len(fileList)) if fileList[i][0].endswith(\".csv\")][0]\n        dbutils.fs.mv(\n            fileList,\n            \"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+dirs[\"test\"]+fileNamePrefix+str(i)+\".csv\"\n        )\n        dbutils.fs.rm(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+dirs[\"test\"]+fileNamePrefix+str(i)+\".csv_buff\",\n                      recurse=True)\ncreateFiles(files,range(4),\"airlines_testFile-\",\"true\")\n\n# corrupted files creation:\n# files[4] -> random number (1-15) of random columns dropped\nrandom.seed(1)\nfiles[4] = files[4].drop(\n    *random.choices(\n        StructType(files[4].schema).fieldNames(),\n        k=random.randint(1,15)\n    )\n)\n\n# files[5] -> random number (1-15) of random columns randomily renamed\ndef getRandString(length):\n    return ''.join(random.choice(string.ascii_letters) for i in range(length))\n\nfor i in range(random.randint(1,15)):\n    files[5] = files[5].withColumnRenamed(\n        random.choice(StructType(files[5].schema).fieldNames()),\n        getRandString(10)\n    )\n    \n# files[6] -> change 'YES'/'NO' in 'IsDepDelayed' column to 'Y'/'N' (assuming: final database is expecting 'YES'/'NO' format)\nmapping = {\"YES\":\"Y\", \"NO\":\"N\"}\nmapping_expr = create_map([lit(x) for x in chain(*mapping.items())])\nfiles[6] = files[6].withColumn(\"IsDepDelayed\",mapping_expr.getItem(files[6].IsDepDelayed))\n\ncreateFiles(files,range(4,7),\"airlines_testFile-\",\"true\")\n\n# files[7] -> file without header\ncreateFiles(files,range(7,8),\"airlines_testFile-\",\"false\")\n\n# files [8] -> is duplicated 'files[6]'\nfiles[8] = files[6]\ncreateFiles(files,range(8,9),\"airlines_testFile-\",\"true\")\n\n# files[9] -> badly named file\ncreateFiles(files,range(9,10),\"airlin_testFile-\",\"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b436c81f-478e-47c9-80b4-bc9770da66ee"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# choose a file from created in previous cell and copy it FROM \"(...)testSourceFiles\" TO \"(...)lvl1-raw\" layer\ncopiedTestFileName = \"airlines_testFile-0.csv\"  # \"airlines_testFile-X.csv\" with X=0,1,2,...,8 and \"airlin_testFile-9.csv\". Corrupted files: X=4,5,...,8 (and 9 -> bad filename).\n\ndbutils.fs.cp(\n    \"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+dirs[\"test\"]+copiedTestFileName,\n    \"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+dirs[\"lvl1\"]+copiedTestFileName,\n    recurse=True\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74215588-a716-4094-b7e8-352ecff9f097"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"buildDataLakeStructure","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1522548383478092}},"nbformat":4,"nbformat_minor":0}
