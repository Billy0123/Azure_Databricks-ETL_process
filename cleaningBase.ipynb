{"cells":[{"cell_type":"code","source":["from fnmatch import fnmatch\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.types import *\n\ndef rejectFile(fileName):\n    return dbutils.fs.mv(\n        \"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+dirs[\"lvl1\"]+fileName,\n        \"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+dirs[\"lvl1-rejected\"]+fileName,\n        recurse=True)\n    return True\n\ndef checkIfNotRejectedDir(filePath):\n    if (filePath+\"/\")!=(container+\"/\"+dirs[\"lvl1\"]):\n        raise Exception(f\"False-trigger, generated by moving files to \\'rejected\\'.\")  # it might be better without generating an exception\n\ndef checkFileNamePattern(fileName, pattern):\n    if not fnmatch(fileName,pattern) or len(fileName.split(\".\"))>2:\n        raise Exception(f\"The filename ({fileName}) does not match the pattern ({rawFileNamePattern}) or contains more than a single \\'.\\'. File moved to \\'rejected\\' (success?: {rejectFile(fileName)}).\")\n        \ndef checkHeader(fileName):\n    header = (spark.read.format(\"csv\")\n              .option(\"header\", \"false\")\n              .load(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+\n                    dirs[\"lvl1\"]+fileName)\n              .limit(1)\n             )\n    row = header.collect()[0]\n    if any(cell.isdigit() for cell in row):\n        raise Exception(f\"File ({fileName}) has no headers. File moved to \\'rejected\\' (success?: {rejectFile(fileName)}).\")\n\ndef getColumnsForCleaning(config, data):\n    dataCols = data.columns\n    for colSettings in config:\n        matchingCols = []\n        patterns = [colSettings.asDict()[\"targetName\"]]+colSettings.asDict()[\"namePatterns\"]\n        for pattern in patterns:\n            matchingCols.append([c for c in dataCols if fnmatch(c.upper(),pattern.upper())])  # assuming: case-insensitivity\n        flattenedMC = [item for sublist in matchingCols for item in sublist]\n        distinctMC = list(set(flattenedMC))\n        if len(distinctMC)==0:\n            if colSettings.asDict()[\"nullable\"]:  # no matching column is found but column is nullable -> add null-filled column to loaded data named as 'targetName'\n                data = data.withColumn(patterns[0], lit(None).cast(colSettings.asDict()[\"type\"]))\n            else: raise Exception(f\"File ({fileName}) has no column matching any pattern ({patterns}) and is set as no-nullable. File moved to \\'rejected\\' (success?: {rejectFile(fileName)}).\")\n        elif len(distinctMC)>1: \n            raise Exception(f\"File ({fileName}) has multiple columns ({distinctMC}) matching the patterns ({patterns}). File moved to \\'rejected\\' (success?: {rejectFile(fileName)}).\")\n        else:\n            data = data.withColumnRenamed(distinctMC[0],patterns[0])\n            data = data.withColumn(patterns[0],col(patterns[0]).cast(colSettings.asDict()[\"type\"]))\n    return data\n\ndef buildSchema(config):\n    mapTypes = {\n        \"Integer\": IntegerType(),\n        \"Long\": LongType(),\n        \"String\": StringType(),\n        \"Float\": FloatType(),\n        \"Double\": DoubleType(),\n        \"Boolean\": BooleanType()\n    }\n    structFieldList = [StructField(conf.asDict()[\"targetName\"],\n                                   mapTypes[conf.asDict()[\"type\"]],\n                                   nullable=conf.asDict()[\"nullable\"])\n                       for conf in config]\n    return StructType(structFieldList)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87a911af-1c50-4be1-a2b2-08607e7106d1"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"cleaningBase","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1260026330619563}},"nbformat":4,"nbformat_minor":0}
