{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql import Window\nfrom pyspark.sql.types import *\n\ndef getMaxID(table):\n    maxID = table.agg({\"ID\": \"max\"}).collect()[0][0]\n    return 0 if maxID is None else maxID\n\ndef unifyWithIncremendedID(table,appendedTable,columns,maxID):\n    return (table.union(appendedTable)\n                       .dropDuplicates(columns)\n                       .withColumn(\"ID\",\n                                   coalesce(\n                                       col(\"ID\"),\n                                       row_number().over(Window.orderBy(\"ID\")) + lit(maxID)\n                                   )\n                                  )\n           )\n\ndef readTable(fileName):\n    return (spark.read.format(\"parquet\")\n            .option(\"header\", \"true\")\n            .option(\"inferSchema\", \"true\")\n            .load(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+\n                  dirs[\"lvl3\"]+fileName+\".parquet\")\n            .cache()\n           )\n    \ndef loadTable(table,fileName):\n    return (table.write\n            .format(\"parquet\")\n            .mode(\"overwrite\")\n            .option(\"header\", \"true\")\n            .save(\"abfss://\"+container+\"@\"+storageAccountName+\".dfs.core.windows.net/\"+\n                  dirs[\"lvl3\"]+fileName+\".parquet\"))\n    \ndef updateDimensionTable(config,data,database):\n    outputTable = readTable(database)\n    maxID = getMaxID(outputTable)\n\n    selectedColumns = [conf.asDict()[\"targetName\"] for conf in config if conf.asDict()[\"targetBase\"]==database]\n    selectedData = (data.withColumn(\"ID\",lit(None).cast(LongType()))\n                            .select([\"ID\"]+selectedColumns)\n                            .cache()\n                   )\n    outputTable = unifyWithIncremendedID(outputTable,selectedData,selectedColumns,maxID)\n    loadTable(outputTable,database)\n    return outputTable\n\ndef updateFactsTable(config,data,databases,dimTables,configFileName):\n    outputTable = readTable(configFileName.split(\".\")[0])\n    maxID = getMaxID(outputTable)\n    \n    factsTable = data\n    for database,dimTable in list(zip(databases,dimTables)):\n        cond = [factsTable[conf.asDict()[\"targetName\"]] == dimTable[conf.asDict()[\"targetName\"]] for conf in config if conf.asDict()[\"targetBase\"]==database]\n        factsTable = factsTable.join(dimTable,cond,\"inner\").withColumnRenamed(\"ID\",database+\"_ID\")\n    selectedColumns = [database+\"_ID\" for database in databases]\n    factsTable = (factsTable.withColumn(\"ID\",lit(None).cast(LongType()))\n                            .select([\"ID\"]+[database+\"_ID\" for database in databases])\n                            .cache()\n                 )\n    outputTable = unifyWithIncremendedID(outputTable,factsTable,selectedColumns,maxID)\n    loadTable(outputTable,configFileName.split(\".\")[0])\n    return outputTable"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2626373-2245-489a-ad4c-57fe30307492"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"loadBase","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2170184853769323}},"nbformat":4,"nbformat_minor":0}
